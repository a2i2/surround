"""
This module defines the tasks that can be executed using `surround run [task name]`
"""

import os
import sys
import subprocess
import re
import webbrowser
import logging
import ssl

from pathlib import Path
from datetime import datetime

from google.oauth2 import service_account
from googleapiclient import discovery
from googleapiclient.errors import HttpError

from surround import Config
from surround.util import generate_docker_volume_path
from surround.experiment.util import get_surround_config

CONFIG = Config(os.path.dirname(__file__))
DOIT_CONFIG = {{'verbosity':2, 'backend':'sqlite3'}}
PACKAGE_PATH = os.path.basename(CONFIG["package_path"])
IMAGE = "%s/%s:%s" % (CONFIG["company"], CONFIG["image"], CONFIG["version"])
IMAGE_JUPYTER = "%s/%s-jupyter:%s" % (CONFIG["company"], CONFIG["image"], CONFIG["version"])
DOCKER_JUPYTER = "Dockerfile.Notebook"

PARAMS = [
    {{
        'name': 'args',
        'long': 'args',
        'type': str,
        'default': ""
    }}
]

def task_status():
    """Show information about the project such as available runners and assemblers"""
    return {{
        'actions': ["%s -m %s --status" % (sys.executable, PACKAGE_PATH)]
    }}

def task_build():
    """Build the Docker image for the current project"""
    return {{
        'actions': ['docker build --tag=%s .' % IMAGE],
        'params': PARAMS
    }}

def task_remove():
    """Remove the Docker image for the current project"""
    return {{
        'actions': ['docker rmi %s -f' % IMAGE],
        'params': PARAMS
    }}

def task_dev():
    """Run the main task for the project"""
    cmd = [
        "docker",
        "run",
        "-p 8080:8080",
        "--volume",
        "\"%s/\":/app" % CONFIG["volume_path"],
        "%s" % IMAGE,
        "python3 -m %s %s" % (PACKAGE_PATH, "%(args)s")
    ]
    return {{
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_interactive():
    """Run the Docker container in interactive mode"""
    def run():
        cmd = [
            'docker',
            'run',
            '-p',
            '8080:8080',
            '-it',
            '--rm',
            '-w',
            '/app',
            '--volume',
            '%s/:/app' % CONFIG['volume_path'],
            IMAGE,
            'bash'
        ]
        process = subprocess.Popen(cmd, encoding='utf-8')
        process.wait()

    return {{
        'actions': [run]
    }}

def task_prod():
    """Run the main task inside a Docker container for use in production """
    cmd = [
        "docker",
        "run",
        "-p 8080:8080",
        IMAGE,
        "python3 -m %s" % PACKAGE_PATH,
        "%(args)s"
    ]

    return {{
        'actions': [" ".join(cmd)],
        'task_dep': ["build"],
        'params': PARAMS
    }}

def task_train():
    """Run training mode inside the container"""
    output_path = CONFIG["volume_path"] + "/output"
    data_path = CONFIG["volume_path"] + "/input"

    global_config = get_surround_config()

    # Inject user's name and email into the env variables of the container
    user_name = global_config.get_path("user.name")
    user_email = global_config.get_path("user.email")
    experiment_args = "-e \"SURROUND_USER_NAME=%s\" " % user_name
    experiment_args += "-e \"SURROUND_USER_EMAIL=%s\"" % user_email

    experiment_path = os.path.join(str(Path.home()), ".experiments")
    experiment_volume_path = generate_docker_volume_path(experiment_path)

    # Ensure experiments will work if using a local storage location (not in the cloud)
    if os.path.join(experiment_path, "local") == global_config.get_path("experiment.url"):
        experiment_args += " --volume \"%s\":/experiments " % experiment_volume_path
        experiment_args += "-e \"SURROUND_EXPERIMENT_URL=/experiments/local\""
    else:
        current_url = global_config.get_path("experiment.url")
        experiment_args += " -e \"SURROUND_EXPERIMENT_URL=%s\"" % current_url

    cmd = [
        "docker run %s" % experiment_args,
        "--volume \"%s\":/app/output" % output_path,
        "--volume \"%s\":/app/input" % data_path,
        "%s" % IMAGE,
        "python3 -m {project_name} --mode train %(args)s"
    ]

    return {{
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_batch():
    """Run batch mode inside the container"""
    output_path = CONFIG["volume_path"] + "/output"
    data_path = CONFIG["volume_path"] + "/input"

    global_config = get_surround_config()

    # Inject user's name and email into the env variables of the container
    user_name = global_config.get_path("user.name")
    user_email = global_config.get_path("user.email")
    experiment_args = "-e \"SURROUND_USER_NAME=%s\" " % user_name
    experiment_args += "-e \"SURROUND_USER_EMAIL=%s\"" % user_email

    experiment_path = os.path.join(str(Path.home()), ".experiments")
    experiment_volume_path = generate_docker_volume_path(experiment_path)

    # Ensure experiments will work if using a local storage location (not in the cloud)
    if os.path.join(experiment_path, "local") == global_config.get_path("experiment.url"):
        experiment_args += " --volume \"%s\":/experiments " % experiment_volume_path
        experiment_args += "-e \"SURROUND_EXPERIMENT_URL=/experiments/local\""
    else:
        current_url = global_config.get_path("experiment.url")
        experiment_args += " -e \"SURROUND_EXPERIMENT_URL=%s\"" % current_url

    cmd = [
        "docker run %s" % experiment_args,
        "--volume \"%s\":/app/output" % output_path,
        "--volume \"%s\":/app/input" % data_path,
        "%s" % IMAGE,
        "python3 -m {project_name} --mode batch %(args)s"
    ]

    return {{
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_train_local():
    """Run training mode locally"""
    cmd = [
        sys.executable,
        "-m %s" % PACKAGE_PATH,
        "--mode train",
        "--runner 1",
        "%(args)s"
    ]

    return {{
        'basename': 'trainLocal',
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_batch_local():
    """Run batch mode locally"""
    cmd = [
        sys.executable,
        "-m %s" % PACKAGE_PATH,
        "--mode batch",
        "--runner 1",
        "%(args)s"
    ]

    return {{
        'basename': 'batchLocal',
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_web():
    """Run web mode inside the container"""
    cmd = [
        "docker",
        "run",
        "-p",
        "8080:8080",
        IMAGE,
        "python3 -m %s" % PACKAGE_PATH,
        "--runner WebRunner",
        "%(args)s"
    ]

    return {{
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_web_local():
    """Run web mode locally"""
    cmd = [
        sys.executable,
        "-m %s" % PACKAGE_PATH,
        "--runner WebRunner",
        "%(args)s"
    ]

    return {{
        'basename': 'webLocal',
        'actions': [" ".join(cmd)],
        'params': PARAMS
    }}

def task_build_jupyter():
    """Build the Docker image for a Jupyter Lab notebook"""
    return {{
        'basename': 'buildJupyter',
        'actions': ['docker build --tag=%s . -f %s' % (IMAGE_JUPYTER, DOCKER_JUPYTER)],
        'task_dep': ['build'],
        'params': PARAMS
    }}

def compute_docker_login():
    """Login to the Google Cloud Container Registry with Docker"""

    if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
        LOGGER.info("You have not provided any Google Cloud credentials!")
        LOGGER.info("Set GOOGLE_APPLICATION_CREDENTIALS env var to the credentials file location")
        return False

    with open(os.environ['GOOGLE_APPLICATION_CREDENTIALS'], 'rb') as cred_file:
        process = subprocess.Popen(
            [
                'docker',
                'login',
                '-u',
                '_json_key',
                '--password-stdin',
                'https://gcr.io'
            ], stdin=cred_file)
        return process.wait() == 0

def compute_get_credentials():
    """Returns the Google Cloud service account"""

    if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
        LOGGER.info("You have not provided any Google Cloud credentials!")
        LOGGER.info("Set GOOGLE_APPLICATION_CREDENTIALS env var to the credentials file location")

        return None

    # Load service account from credentials json
    credentials = os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
    credentials = service_account.Credentials.from_service_account_file(
        credentials,
        scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )

    return credentials

def compute_docker_build_cmd(dockerfile, is_gpu=False):
    """Constructs the build command and returns it and the image URI"""

    # Get configuration from global/local config
    global_config = get_surround_config()
    user_name = global_config.get_path("user.name")
    user_email = global_config.get_path("user.email")
    experiment_url = global_config.get_path("experiment.url")

    credentials = compute_get_credentials()
    if not credentials:
        # Fail task since we have no credentials
        return "", lambda: False

    # Copy the credentials file to the root of the project
    with open(os.environ["GOOGLE_APPLICATION_CREDENTIALS"], "rb") as cred_file:
        with open("credentials.json", "wb+") as output:
            output.write(cred_file.read())

    # Construct image uri
    image_uri = "gcr.io/%s/%s" % (credentials.project_id, IMAGE if not is_gpu else "%s-gpu" % IMAGE)

    # Construct build command (and clean-up)
    build_cmd = "docker build -f %s " % dockerfile
    build_cmd += "--build-arg \"EXPERIMENT_URL=%s\" " % experiment_url
    build_cmd += "--build-arg \"USER_NAME=%s\" " % user_name
    build_cmd += "--build-arg \"USER_EMAIL=%s\" " % user_email
    build_cmd += "-t %s " % image_uri
    build_cmd += "./"

    return image_uri, build_cmd

def task_compute_build():
    """Build compute image and push to Google Cloud (Container Registry)"""

    # Construct build command
    image_uri, build_cmd = compute_docker_build_cmd("Dockerfile-compute")

    # Construct push command
    push_cmd = "docker push %s" % image_uri

    return {{
        'basename': 'buildCompute',
        'actions': [compute_docker_login, build_cmd, 'rm credentials.json', push_cmd]
    }}

def task_compute_build_gpu():
    """Build GPU compute image and push to Google Cloud (Container Registry)"""

    # Construct build command
    image_uri, build_cmd = compute_docker_build_cmd("Dockerfile-compute-gpu", True)

    # Construct push command
    push_cmd = "docker push %s" % image_uri

    return {{
        'basename': 'buildComputeGPU',
        'actions': [compute_docker_login, build_cmd, 'rm credentials.json', push_cmd]
    }}

def compute_check_image(is_gpu=False):
    """Checks whether there has been an image build for compute"""

    image_name = IMAGE if not is_gpu else ("%s-gpu" % IMAGE)
    process = subprocess.Popen(
        [
            'docker',
            'images',
            '-q',
            '-f',
            'reference=gcr.io/*/%s' % image_name
        ], stdout=subprocess.PIPE)
    process.wait()

    if not process.stdout.read():
        LOGGER.info("No compute image found!")
        LOGGER.info("Make sure you run the following before deploying to the cloud!")
        LOGGER.info("$ surround run %s", "buildComputeGPU" if is_gpu else "buildCompute")
        return False

    return True

def compute_get_ml_client(credentials):
    """Gets the ML API client from Google using the credentials.
    Tries multiple times due to random Google bug"""

    count = 0
    cloud_ml = None
    while count < 5:
        try:
            # python api for AI Platform
            cloud_ml = discovery.build('ml', 'v1', credentials=credentials)
        except ssl.SSLEOFError:
            count += 1
            continue

        break

    if not cloud_ml:
        LOGGER.info("Failed to create ML client, try again later!")

    return cloud_ml

def compute_deploy_job(mode="batch", is_gpu=False):
    """Deploys a compute job to Google Cloud (AI Platform)"""

    credentials = compute_get_credentials()
    if not credentials:
        return False

    cloud_ml = compute_get_ml_client(credentials)
    if not cloud_ml:
        return False

    # get current time stamp
    now = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Generate configuration
    project_id = 'projects/%s' % credentials.project_id
    job_name = '{project_name}_%s_%s_%s' % (mode, ("gpu" if is_gpu else "cpu"), now)
    image_name = IMAGE if not is_gpu else ("%s-gpu" % IMAGE)
    image_uri = "gcr.io/%s/%s" % (credentials.project_id, image_name)

    # Only run as experiment if using cloud storage for experiments
    global_config = get_surround_config()
    run_as_experiment = not os.path.exists(global_config.get_path("experiment.url"))

    job_spec = {{
        'jobId': job_name,
        'trainingInput': {{
            "scaleTier": CONFIG["compute"]["gpu" if is_gpu else "cpu"]["scale-tier"],
            "masterConfig": {{
                "imageUri": image_uri
            }},
            "args": [
                '--mode=%s' % mode,
                '--runner=1',
                '' if run_as_experiment else "--no-experiment"
            ],
            "region": CONFIG["compute"]["gpu" if is_gpu else "cpu"]["region"]
        }}
    }}

    # Request for creating job
    # pylint: disable=no-member
    request = cloud_ml.projects().jobs().create(body=job_spec, parent=project_id)

    try:
        # execute request
        job = request.execute()

        LOGGER.info("Job deployed to cloud successfully!\n")
        LOGGER.info("Job ID:     %s", job['jobId'])
        LOGGER.info("Status:     %s\n", job['state'])
        LOGGER.info("To check the status, run:")
        LOGGER.info("$ surround run statusCompute %s", job['jobId'])
    except HttpError as err:
        LOGGER.info("Failed to deploy job to the cloud!")
        # pylint: disable=protected-access
        LOGGER.info("Reason: %s", err._get_reason())

    return True

def task_compute_batch():
    """Submit a batch-predict compute job to Google Cloud (AI Platform)"""

    return {{
        'basename': 'batchCompute',
        'actions': [compute_check_image, compute_deploy_job]
    }}

def task_compute_train():
    """Submit a training compute job to Google Cloud (AI Platform)"""

    return {{
        'basename': 'trainCompute',
        'actions': [compute_check_image, lambda: compute_deploy_job(mode="train")]
    }}

def task_compute_batch_gpu():
    """Submit a batch-predict GPU compute job to Google Cloud (AI Platform)"""

    return {{
        'basename': 'batchComputeGPU',
        'actions': [lambda: compute_check_image(True), lambda: compute_deploy_job(is_gpu=True)]
    }}

def task_compute_train_gpu():
    """Submit a training GPU compute job to Google Cloud (AI Platform)"""

    return {{
        'basename': 'trainComputeGPU',
        'actions': [
            lambda: compute_check_image(True),
            lambda: compute_deploy_job(mode="train", is_gpu=True)
        ]
    }}

def task_compute_list():
    """List compute jobs history from Google Cloud (AI Platform)"""

    def print_jobs():
        """Gets and prints the jobs from Google Cloud"""

        # Get the credentials file from the environment
        credentials = compute_get_credentials()
        if not credentials:
            return False

        cloud_ml = compute_get_ml_client(credentials)
        if not cloud_ml:
            return False

        project_id = 'projects/%s' % credentials.project_id
        # pylint: disable=no-member
        request = cloud_ml.projects().jobs().list(parent=project_id)
        response = request.execute()

        pattern = r'^{project_name}_(train|batch)_(cpu|gpu)'
        for job in [j for j in response['jobs'] if re.search(pattern, j['jobId'])]:
            scale_tier = "BASIC"
            if 'scaleTier' in job['trainingInput']:
                scale_tier = job['trainingInput']['scaleTier']

            LOGGER.info("Job ID:     %s", job['jobId'])
            LOGGER.info("Status:     %s", job['state'])
            LOGGER.info("Scale Tier: %s", scale_tier)
            LOGGER.info("Arguments:  %s\n", job['trainingInput']['args'])

        return True

    return {{
        'basename': 'listCompute',
        'actions': [print_jobs]
    }}

def task_compute_status():
    """Retrieve the status of a single compute job from Google Cloud (AI Platform)"""

    def show_status(args):
        """Shows the status of the job"""

        # Get the credentials file from the environment
        credentials = compute_get_credentials()
        if not credentials:
            return False

        job_id = 'projects/%s/jobs/%s' % (credentials.project_id, args)

        cloud_ml = compute_get_ml_client(credentials)
        if not cloud_ml:
            return False

        try:
            # pylint: disable=no-member
            request = cloud_ml.projects().jobs().get(name=job_id)
            response = request.execute()

            scale_tier = "BASIC"
            if 'scaleTier' in response['trainingInput']:
                scale_tier = response['trainingInput']['scaleTier']

            LOGGER.info("Job ID:     %s", response['jobId'])
            LOGGER.info("Status:     %s", response['state'])
            LOGGER.info("Scale Tier: %s", scale_tier)
            LOGGER.info("Arguments:  %s", response['trainingInput']['args'])
        except HttpError:
            LOGGER.info("Failed to get the status of the job!")

        return True

    return {{
        'basename': 'statusCompute',
        'actions': [show_status],
        'params': PARAMS
    }}

def task_compute_kill():
    """Kills a running compute job on Google Cloud (AI Platform)"""

    def kill(args):
        # Get the credentials file from the environment
        credentials = compute_get_credentials()
        if not credentials:
            return False

        job_id = 'projects/%s/jobs/%s' % (credentials.project_id, args)

        cloud_ml = compute_get_ml_client(credentials)
        if not cloud_ml:
            return False

        try:
            # pylint: disable=no-member
            request = cloud_ml.projects().jobs().cancel(name=job_id)
            request.execute()

            LOGGER.info("Sent the kill request!\n")
            LOGGER.info("Check the status using:")
            LOGGER.info("$ surround run statusCompute %s", args)
        except HttpError:
            LOGGER.info("Failed to send cancel job request!")
            LOGGER.info("Check the job ID and whether it is still running or not")

        return True

    return {{
        'basename': 'killCompute',
        'actions': [kill],
        'params': PARAMS
    }}

def task_jupyter():
    """Run a Jupyter Lab notebook"""
    cmd = [
        "docker",
        "run",
        "-itp",
        "8888:8888",
        '-w',
        '/app',
        "--volume",
        "\"%s/\":/app" % CONFIG["volume_path"],
        IMAGE_JUPYTER
    ]
    return {{
        'actions': [" ".join(cmd)],
    }}
